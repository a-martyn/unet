{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from utils import *\n",
    "from model.data_loader import *\n",
    "from model.unet_baseline import unet\n",
    "from model.unet_ternaus import ternausNet16\n",
    "from model.unet_ternaus_tweaked import ternausNet16_tweaked\n",
    "from model.unet_pix2pix import unet_pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pth = 'data/membrane/train'\n",
    "test_pth = 'data/membrane/test'\n",
    "results_pth = 'results/'\n",
    "pretrained_pth = 'pretrained/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Train the original unet model on the full 512px dataset until overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sz = (512, 512, 1)\n",
    "batch_sz = 2\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "    ['unet_baseline', unet, dict(input_size=img_sz, transpose=True)]\n",
    "]\n",
    "\n",
    "# Data loaders\n",
    "train_loader = loader(train_pth, input_generator_train, target_generator_train, \n",
    "                      batch_sz=batch_sz, img_sz=img_sz[:2])\n",
    "test_loader = loader(test_pth, input_generator_test, target_generator_test, \n",
    "                     batch_sz=batch_sz, img_sz=img_sz[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_baseline\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 259s 519ms/step - loss: 0.5889 - acc: 0.8872 - val_loss: 0.4561 - val_acc: 0.9299\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 257s 515ms/step - loss: 0.4870 - acc: 0.9201 - val_loss: 0.4700 - val_acc: 0.9347\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 257s 514ms/step - loss: 0.4551 - acc: 0.9270 - val_loss: 0.4470 - val_acc: 0.9349\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 257s 514ms/step - loss: 0.4327 - acc: 0.9318 - val_loss: 0.4386 - val_acc: 0.9376\n",
      "Epoch 5/10\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.9358\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "500/500 [==============================] - 255s 511ms/step - loss: 0.4139 - acc: 0.9358 - val_loss: 0.4374 - val_acc: 0.9367\n",
      "Epoch 6/10\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.9396\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "500/500 [==============================] - 255s 511ms/step - loss: 0.4003 - acc: 0.9396 - val_loss: 0.4717 - val_acc: 0.9371\n"
     ]
    }
   ],
   "source": [
    "test_title = '512px_500steps_10epochs'\n",
    "\n",
    "training_params = dict(\n",
    "    train_steps=500, \n",
    "    val_steps=100, \n",
    "    epochs=10, \n",
    "    iterations=1, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    save_pth = f'{pretrained_pth}{model[0]}_{test_title}.h5'\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, \n",
    "                         model_params=model[2], save_pth=save_pth)\n",
    "    results_df = hists2df(results)\n",
    "    results_df.to_csv(f'{results_pth}{model[0]}_{test_title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Validation Accuracy: 0.9376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Test: 20 epochs of 250 training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sz = (256, 256, 1)\n",
    "batch_sz = 2\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "#     ['unet_baseline',           unet,                 dict(input_size=img_sz, transpose=True)],\n",
    "#     ['unet_baseline_upsampled', unet,                 dict(input_size=img_sz, transpose=False)],\n",
    "#     ['unet_ternaus_nopre',      ternausNet16_tweaked, dict(input_size=img_sz, dropout=False, batch_norm=False, pretrained=False)],\n",
    "#     ['unet_ternaus',            ternausNet16,         dict(input_size=img_sz)],\n",
    "#     ['unet_ternaus_drop',       ternausNet16_tweaked, dict(input_size=img_sz, dropout=True,  batch_norm=False, pretrained=True)],\n",
    "#     ['unet_ternaus_bn',         ternausNet16_tweaked, dict(input_size=img_sz, dropout=False, batch_norm=True,  pretrained=True)],\n",
    "#     ['unet_ternaus_dropbn',     ternausNet16_tweaked, dict(input_size=img_sz, dropout=True,  batch_norm=True,  pretrained=True)],\n",
    "    ['unet_pix2pix',            unet_pix2pix_pytorch, dict(input_size=img_sz)]\n",
    "]\n",
    "\n",
    "# Data loaders\n",
    "train_loader = loader(train_pth, input_generator_train, target_generator_train, batch_sz=batch_sz, img_sz=img_sz[:2])\n",
    "test_loader = loader(test_pth, input_generator_test, target_generator_test, batch_sz=batch_sz, img_sz=img_sz[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_baseline\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 0.6587 - acc: 0.8478 - val_loss: 0.4550 - val_acc: 0.9018\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 45s 182ms/step - loss: 0.4809 - acc: 0.8967 - val_loss: 0.3688 - val_acc: 0.9232\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 46s 182ms/step - loss: 0.4134 - acc: 0.9112 - val_loss: 0.3438 - val_acc: 0.9276\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 0.3821 - acc: 0.9174 - val_loss: 0.3147 - val_acc: 0.9348\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 0.3572 - acc: 0.9226 - val_loss: 0.3164 - val_acc: 0.9357\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.9264\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.3384 - acc: 0.9264 - val_loss: 0.3221 - val_acc: 0.9336\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9309\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.3172 - acc: 0.9309 - val_loss: 0.3211 - val_acc: 0.9339\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 0.7734 - acc: 0.8001 - val_loss: 0.5674 - val_acc: 0.8861\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.5794 - acc: 0.8762 - val_loss: 0.3937 - val_acc: 0.9172\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.4501 - acc: 0.9011 - val_loss: 0.3744 - val_acc: 0.9234\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.4019 - acc: 0.9117 - val_loss: 0.4649 - val_acc: 0.9246\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.3730 - acc: 0.9181 - val_loss: 0.3809 - val_acc: 0.9281\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 0.3479 - acc: 0.9235 - val_loss: 0.3414 - val_acc: 0.9340\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 47s 190ms/step - loss: 0.3315 - acc: 0.9272 - val_loss: 0.3613 - val_acc: 0.9344\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 47s 190ms/step - loss: 0.3171 - acc: 0.9305 - val_loss: 0.3509 - val_acc: 0.9353\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 0.3066 - acc: 0.9328 - val_loss: 0.3535 - val_acc: 0.9360\n",
      "Epoch 10/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9349\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 0.2973 - acc: 0.9349 - val_loss: 0.3879 - val_acc: 0.9337\n",
      "Epoch 11/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9378\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 0.2839 - acc: 0.9377 - val_loss: 0.3863 - val_acc: 0.9352\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.6579 - acc: 0.8471 - val_loss: 0.4669 - val_acc: 0.8986\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 46s 186ms/step - loss: 0.4759 - acc: 0.8974 - val_loss: 0.3697 - val_acc: 0.9230\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 47s 186ms/step - loss: 0.4125 - acc: 0.9112 - val_loss: 0.3361 - val_acc: 0.9328\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.9175\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.3810 - acc: 0.9176 - val_loss: 0.3328 - val_acc: 0.9322\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.9229\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.3565 - acc: 0.9228 - val_loss: 0.3365 - val_acc: 0.9325\n",
      "Epoch 00005: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 50s 199ms/step - loss: 0.6767 - acc: 0.8359 - val_loss: 0.5010 - val_acc: 0.8892\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 46s 186ms/step - loss: 0.4936 - acc: 0.8936 - val_loss: 0.3695 - val_acc: 0.9251\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.4233 - acc: 0.9092 - val_loss: 0.3263 - val_acc: 0.9332\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.9163\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.3884 - acc: 0.9163 - val_loss: 0.3372 - val_acc: 0.9319\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.3618 - acc: 0.9217 - val_loss: 0.3313 - val_acc: 0.9339\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 0.3551 - acc: 0.9231 - val_loss: 0.3281 - val_acc: 0.9345\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.9240\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.3502 - acc: 0.9241 - val_loss: 0.3424 - val_acc: 0.9327\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.9245\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.3478 - acc: 0.9246 - val_loss: 0.3351 - val_acc: 0.9337\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 0.6941 - acc: 0.8392 - val_loss: 0.5405 - val_acc: 0.8995\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.5780 - acc: 0.8914 - val_loss: 0.4746 - val_acc: 0.9255\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.5224 - acc: 0.9084 - val_loss: 0.4654 - val_acc: 0.9298\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.9141\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.4864 - acc: 0.9141 - val_loss: 0.3433 - val_acc: 0.9295\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.3668 - acc: 0.9203 - val_loss: 0.3291 - val_acc: 0.9333\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.9221\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3582 - acc: 0.9220 - val_loss: 0.3409 - val_acc: 0.9323\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.9231\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3532 - acc: 0.9231 - val_loss: 0.3431 - val_acc: 0.9322\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "TESTING MODEL: unet_baseline_upsampled\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 57s 227ms/step - loss: 0.7010 - acc: 0.8451 - val_loss: 0.5189 - val_acc: 0.9053\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.5638 - acc: 0.8975 - val_loss: 0.4690 - val_acc: 0.9284\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.5138 - acc: 0.9114 - val_loss: 0.4597 - val_acc: 0.9334\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.4901 - acc: 0.9167 - val_loss: 0.4503 - val_acc: 0.9345\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.4708 - acc: 0.9212 - val_loss: 0.4538 - val_acc: 0.9371\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.9249\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.4548 - acc: 0.9248 - val_loss: 0.4473 - val_acc: 0.9371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.9287\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.4437 - acc: 0.9287 - val_loss: 0.4647 - val_acc: 0.9356\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.9294\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.4383 - acc: 0.9294 - val_loss: 0.4616 - val_acc: 0.9360\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.6044 - acc: 0.8650 - val_loss: 0.3995 - val_acc: 0.9130\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.4385 - acc: 0.9062 - val_loss: 0.3295 - val_acc: 0.9303\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.3946 - acc: 0.9150 - val_loss: 0.3233 - val_acc: 0.9353\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3688 - acc: 0.9202\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3686 - acc: 0.9203 - val_loss: 0.3445 - val_acc: 0.9351\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3409 - acc: 0.9261 - val_loss: 0.3233 - val_acc: 0.9359\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3349 - acc: 0.9273 - val_loss: 0.3224 - val_acc: 0.9364\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.9285\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3283 - acc: 0.9285 - val_loss: 0.3273 - val_acc: 0.9360\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9293\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3249 - acc: 0.9293 - val_loss: 0.3299 - val_acc: 0.9359\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.7158 - acc: 0.8345 - val_loss: 0.5157 - val_acc: 0.9070\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.4755 - acc: 0.8976 - val_loss: 0.3912 - val_acc: 0.9226\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3994 - acc: 0.9136 - val_loss: 0.3932 - val_acc: 0.9288\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3718 - acc: 0.9194 - val_loss: 0.3458 - val_acc: 0.9316\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3508 - acc: 0.9236 - val_loss: 0.2979 - val_acc: 0.9387\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.9280\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3297 - acc: 0.9281 - val_loss: 0.3696 - val_acc: 0.9312\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9328\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3075 - acc: 0.9328 - val_loss: 0.3320 - val_acc: 0.9372\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 1.3868 - acc: 0.7749 - val_loss: 1.3721 - val_acc: 0.7968\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 1.3662 - acc: 0.7802 - val_loss: 1.3579 - val_acc: 0.7971\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.3532 - acc: 0.7798\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 52s 210ms/step - loss: 1.3532 - acc: 0.7797 - val_loss: 1.3443 - val_acc: 0.7965\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.3462 - acc: 0.7792\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 1.3462 - acc: 0.7791 - val_loss: 1.3429 - val_acc: 0.7968\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.7659 - acc: 0.8229 - val_loss: 0.5467 - val_acc: 0.8967\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.5518 - acc: 0.8874 - val_loss: 0.3516 - val_acc: 0.9261\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.4174 - acc: 0.9102 - val_loss: 0.3612 - val_acc: 0.9274\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3799 - acc: 0.9176 - val_loss: 0.2945 - val_acc: 0.9386\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.9229\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3538 - acc: 0.9229 - val_loss: 0.3378 - val_acc: 0.9345\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9279\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3303 - acc: 0.9279 - val_loss: 0.3563 - val_acc: 0.9350\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "TESTING MODEL: unet_pix2pix_paper\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 8.4348 - acc: 0.6303 - val_loss: 6.2421 - val_acc: 0.7027\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 5.0307 - acc: 0.7269 - val_loss: 4.9983 - val_acc: 0.7372\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 4.4297 - acc: 0.7428 - val_loss: 4.2083 - val_acc: 0.7602\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.9581 - acc: 0.7539 - val_loss: 3.8444 - val_acc: 0.7696\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 3.5942 - acc: 0.7626 - val_loss: 3.5791 - val_acc: 0.7751\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 3.4465 - acc: 0.7658 - val_loss: 3.3442 - val_acc: 0.7806\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 3.4781 - acc: 0.7661\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 3.4778 - acc: 0.7662 - val_loss: 3.4440 - val_acc: 0.7802\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 3.4218 - acc: 0.7665 - val_loss: 3.4274 - val_acc: 0.7808\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.3655 - acc: 0.7679 - val_loss: 3.3237 - val_acc: 0.7823\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 3.3169 - acc: 0.7687 - val_loss: 3.2970 - val_acc: 0.7835\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.2869 - acc: 0.7692 - val_loss: 3.2879 - val_acc: 0.7841\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.2451 - acc: 0.7709 - val_loss: 3.2125 - val_acc: 0.7850\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.2076 - acc: 0.7716 - val_loss: 3.2533 - val_acc: 0.7853\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.1676 - acc: 0.7727 - val_loss: 3.2116 - val_acc: 0.7867\n",
      "Epoch 15/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 3.1485 - acc: 0.7727\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 3.1488 - acc: 0.7727 - val_loss: 3.2051 - val_acc: 0.7859\n",
      "Epoch 16/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 3.1596 - acc: 0.7718\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 3.1593 - acc: 0.7719 - val_loss: 3.1998 - val_acc: 0.7862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 6.9779 - acc: 0.6697 - val_loss: 5.7121 - val_acc: 0.7141\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 4.3481 - acc: 0.7453 - val_loss: 4.6322 - val_acc: 0.7528\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 3.8879 - acc: 0.7617 - val_loss: 3.7796 - val_acc: 0.7744\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.5173 - acc: 0.7708 - val_loss: 3.3347 - val_acc: 0.7847\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 3.1563 - acc: 0.7774 - val_loss: 3.0746 - val_acc: 0.7911\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.0929 - acc: 0.7792 - val_loss: 3.0253 - val_acc: 0.7915\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.8560 - acc: 0.7858 - val_loss: 2.8308 - val_acc: 0.8001\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.4089 - acc: 0.7973 - val_loss: 2.2922 - val_acc: 0.8139\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.3225 - acc: 0.7996 - val_loss: 2.2971 - val_acc: 0.8161\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 2.1261 - acc: 0.8046 - val_loss: 2.0527 - val_acc: 0.8219\n",
      "Epoch 11/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.1978 - acc: 0.8028\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 32s 126ms/step - loss: 2.1971 - acc: 0.8029 - val_loss: 2.0438 - val_acc: 0.8184\n",
      "Epoch 12/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.1290 - acc: 0.8033\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 2.1304 - acc: 0.8032 - val_loss: 2.0802 - val_acc: 0.8184\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 8.7370 - acc: 0.6279 - val_loss: 7.4546 - val_acc: 0.6775\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 6.0307 - acc: 0.7057 - val_loss: 5.5203 - val_acc: 0.7255\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 4.5323 - acc: 0.7429 - val_loss: 4.0472 - val_acc: 0.7665\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 3.9026 - acc: 0.7608\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 3.9043 - acc: 0.7607 - val_loss: 7.4553 - val_acc: 0.6739\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 4.2336 - acc: 0.7546 - val_loss: 4.0628 - val_acc: 0.7679\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 4.1282 - acc: 0.7557\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 4.1274 - acc: 0.7558 - val_loss: 4.1184 - val_acc: 0.7671\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 4.0581 - acc: 0.7578 - val_loss: 4.0013 - val_acc: 0.7693\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 4.0515 - acc: 0.7576\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 4.0516 - acc: 0.7576 - val_loss: 4.3380 - val_acc: 0.7618\n",
      "Epoch 9/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 4.0431 - acc: 0.7579\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 4.0426 - acc: 0.7580 - val_loss: 3.9999 - val_acc: 0.7692\n",
      "Epoch 00009: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 11.9146 - acc: 0.5528 - val_loss: 9.8389 - val_acc: 0.6149\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 8.5764 - acc: 0.6413 - val_loss: 7.7600 - val_acc: 0.6712\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 6.9103 - acc: 0.6837 - val_loss: 6.3462 - val_acc: 0.7083\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 5.6515 - acc: 0.7155 - val_loss: 5.2736 - val_acc: 0.7331\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 4.7476 - acc: 0.7361 - val_loss: 4.6136 - val_acc: 0.7500\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 4.1999 - acc: 0.7512 - val_loss: 4.3456 - val_acc: 0.7603\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 4.3579 - acc: 0.7463 - val_loss: 3.7603 - val_acc: 0.7721\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.5496 - acc: 0.7654 - val_loss: 3.0877 - val_acc: 0.7876\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 3.0884 - acc: 0.7777 - val_loss: 3.1722 - val_acc: 0.7879\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 2.9005 - acc: 0.7834 - val_loss: 2.5823 - val_acc: 0.8016\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 2.5864 - acc: 0.7900 - val_loss: 2.4682 - val_acc: 0.8042\n",
      "Epoch 12/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.5974 - acc: 0.7904\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 32s 126ms/step - loss: 2.5987 - acc: 0.7904 - val_loss: 2.6560 - val_acc: 0.8031\n",
      "Epoch 13/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.7796 - acc: 0.7897\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 2.7794 - acc: 0.7897 - val_loss: 2.6523 - val_acc: 0.8040\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 8.4685 - acc: 0.6401 - val_loss: 6.5704 - val_acc: 0.7031\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 5.4529 - acc: 0.7267 - val_loss: 4.9774 - val_acc: 0.7466\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 4.4980 - acc: 0.7488 - val_loss: 4.5996 - val_acc: 0.7574\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 3.8077 - acc: 0.7667 - val_loss: 3.5963 - val_acc: 0.7819\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 3.5171 - acc: 0.7749 - val_loss: 3.2437 - val_acc: 0.7911\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 3.0735 - acc: 0.7862 - val_loss: 2.7407 - val_acc: 0.8015\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 2.7568 - acc: 0.7899 - val_loss: 2.5792 - val_acc: 0.8051\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 2.3969 - acc: 0.7973 - val_loss: 2.2572 - val_acc: 0.8122\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 2.1518 - acc: 0.8025 - val_loss: 2.0381 - val_acc: 0.8179\n",
      "Epoch 10/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.1140 - acc: 0.8035\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 2.1137 - acc: 0.8036 - val_loss: 2.1137 - val_acc: 0.8145\n",
      "Epoch 11/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.1581 - acc: 0.8011\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 31s 126ms/step - loss: 2.1587 - acc: 0.8011 - val_loss: 2.0452 - val_acc: 0.8157\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "TESTING MODEL: unet_pix2pix_pytorch\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 32s 126ms/step - loss: 2.5654 - acc: 0.6558 - val_loss: 0.9691 - val_acc: 0.7900\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.9462 - acc: 0.7813 - val_loss: 0.8349 - val_acc: 0.8074\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.8249 - acc: 0.8030 - val_loss: 0.7279 - val_acc: 0.8314\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.7354 - acc: 0.8256 - val_loss: 0.6547 - val_acc: 0.8535\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.6773 - acc: 0.8418 - val_loss: 0.6079 - val_acc: 0.8677\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.6366 - acc: 0.8537 - val_loss: 0.5660 - val_acc: 0.8755\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.6095 - acc: 0.8615 - val_loss: 0.5443 - val_acc: 0.8824\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5873 - acc: 0.8678 - val_loss: 0.5243 - val_acc: 0.8873\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.5715 - acc: 0.8722 - val_loss: 0.4993 - val_acc: 0.8899\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5555 - acc: 0.8768 - val_loss: 0.4838 - val_acc: 0.8948\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5411 - acc: 0.8806 - val_loss: 0.4646 - val_acc: 0.8991\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5278 - acc: 0.8843 - val_loss: 0.4475 - val_acc: 0.9009\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5138 - acc: 0.8877 - val_loss: 0.4325 - val_acc: 0.9042\n",
      "Epoch 14/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8905\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.5018 - acc: 0.8905 - val_loss: 0.4414 - val_acc: 0.9004\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.4915 - acc: 0.8932 - val_loss: 0.4254 - val_acc: 0.9046\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4907 - acc: 0.8934 - val_loss: 0.4245 - val_acc: 0.9050\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4876 - acc: 0.8940 - val_loss: 0.4175 - val_acc: 0.9077\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4867 - acc: 0.8942 - val_loss: 0.4146 - val_acc: 0.9080\n",
      "Epoch 19/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.8946\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4857 - acc: 0.8945 - val_loss: 0.4195 - val_acc: 0.9063\n",
      "Epoch 20/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8948\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 0.4843 - acc: 0.8947 - val_loss: 0.4216 - val_acc: 0.9053\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 32s 128ms/step - loss: 25.1177 - acc: 0.2208 - val_loss: 25.6759 - val_acc: 0.2035\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 25.1285 - acc: 0.2205\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 25.1268 - acc: 0.2205 - val_loss: 25.6924 - val_acc: 0.2030\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 25.1174 - acc: 0.2208\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 25.1172 - acc: 0.2208 - val_loss: 25.6794 - val_acc: 0.2034\n",
      "Epoch 00003: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 4.8363 - acc: 0.5469 - val_loss: 1.2394 - val_acc: 0.7665\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 1.2561 - acc: 0.7576 - val_loss: 1.1373 - val_acc: 0.7771\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 1.1634 - acc: 0.7654 - val_loss: 1.0906 - val_acc: 0.7806\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 1.1114 - acc: 0.7666 - val_loss: 1.0313 - val_acc: 0.7836\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 1.0659 - acc: 0.7693 - val_loss: 0.9911 - val_acc: 0.7867\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 1.0244 - acc: 0.7728 - val_loss: 0.9535 - val_acc: 0.7904\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.9850 - acc: 0.7771 - val_loss: 0.9189 - val_acc: 0.7945\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.9493 - acc: 0.7811 - val_loss: 0.8810 - val_acc: 0.7996\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.9072 - acc: 0.7874 - val_loss: 0.8376 - val_acc: 0.8078\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.8600 - acc: 0.7955 - val_loss: 0.7743 - val_acc: 0.8205\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.8082 - acc: 0.8072 - val_loss: 0.7261 - val_acc: 0.8318\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.7624 - acc: 0.8192 - val_loss: 0.6827 - val_acc: 0.8438\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.7248 - acc: 0.8298 - val_loss: 0.6539 - val_acc: 0.8514\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.6973 - acc: 0.8381 - val_loss: 0.6227 - val_acc: 0.8611\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.6764 - acc: 0.8447 - val_loss: 0.6070 - val_acc: 0.8670\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 0.6567 - acc: 0.8504 - val_loss: 0.5893 - val_acc: 0.8721\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.6449 - acc: 0.8537 - val_loss: 0.5788 - val_acc: 0.8753\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.6297 - acc: 0.8582 - val_loss: 0.5652 - val_acc: 0.8779\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.6168 - acc: 0.8613 - val_loss: 0.5564 - val_acc: 0.8807\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.6046 - acc: 0.8644 - val_loss: 0.5536 - val_acc: 0.8824\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 25.1298 - acc: 0.2204 - val_loss: 25.6935 - val_acc: 0.2030\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 25.1453 - acc: 0.2200 - val_loss: 25.6740 - val_acc: 0.2036\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 25.1246 - acc: 0.2206\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 25.1244 - acc: 0.2206 - val_loss: 25.6866 - val_acc: 0.2032\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 25.1089 - acc: 0.2211\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 25.1092 - acc: 0.2211 - val_loss: 25.6761 - val_acc: 0.2035\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 32s 129ms/step - loss: 2.2580 - acc: 0.6486 - val_loss: 0.8195 - val_acc: 0.8062\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.7493 - acc: 0.8197 - val_loss: 0.6310 - val_acc: 0.8622\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.6233 - acc: 0.8585 - val_loss: 0.5537 - val_acc: 0.8814\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5734 - acc: 0.8724 - val_loss: 0.4937 - val_acc: 0.8920\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5401 - acc: 0.8812 - val_loss: 0.4630 - val_acc: 0.9013\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.5136 - acc: 0.8877 - val_loss: 0.4468 - val_acc: 0.9067\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 24s 97ms/step - loss: 0.4899 - acc: 0.8931 - val_loss: 0.4041 - val_acc: 0.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.4709 - acc: 0.8977 - val_loss: 0.3871 - val_acc: 0.9178\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.4569 - acc: 0.9009 - val_loss: 0.3715 - val_acc: 0.9208\n",
      "Epoch 10/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.9042\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.4417 - acc: 0.9042 - val_loss: 0.3692 - val_acc: 0.9196\n",
      "Epoch 11/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.9062\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.4329 - acc: 0.9061 - val_loss: 0.3803 - val_acc: 0.9157\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "TESTING MODEL: unet_ternaus_nopre\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.7093 - acc: 0.8343 - val_loss: 0.4685 - val_acc: 0.8958\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.4745 - acc: 0.8970 - val_loss: 0.3550 - val_acc: 0.9274\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.9131\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.4013 - acc: 0.9131 - val_loss: 0.3679 - val_acc: 0.9273\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 59s 237ms/step - loss: 0.3601 - acc: 0.9214 - val_loss: 0.3452 - val_acc: 0.9327\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.9236\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3494 - acc: 0.9236 - val_loss: 0.3489 - val_acc: 0.9321\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.9249\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3434 - acc: 0.9249 - val_loss: 0.3488 - val_acc: 0.9319\n",
      "Epoch 00006: early stopping\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 0.7137 - acc: 0.8363 - val_loss: 0.5151 - val_acc: 0.8892\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.4879 - acc: 0.8939 - val_loss: 0.4015 - val_acc: 0.9194\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.3967 - acc: 0.9138 - val_loss: 0.3122 - val_acc: 0.9360\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.9213- ETA: 2s - loss: 0.3609 - a\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.3604 - acc: 0.9213 - val_loss: 0.3229 - val_acc: 0.9336\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.9274\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.3303 - acc: 0.9274 - val_loss: 0.3377 - val_acc: 0.9331\n",
      "Epoch 00005: early stopping\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.7196 - acc: 0.8339 - val_loss: 0.4865 - val_acc: 0.8921\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.4774 - acc: 0.8964 - val_loss: 0.3492 - val_acc: 0.9290\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.4015 - acc: 0.9133 - val_loss: 0.3159 - val_acc: 0.9349\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.9206\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3641 - acc: 0.9207 - val_loss: 0.3268 - val_acc: 0.9337\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9272\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3321 - acc: 0.9272 - val_loss: 0.3355 - val_acc: 0.9347\n",
      "Epoch 00005: early stopping\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 0.7314 - acc: 0.8299 - val_loss: 0.5458 - val_acc: 0.8783\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.4954 - acc: 0.8924 - val_loss: 0.3517 - val_acc: 0.9264\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.4095 - acc: 0.9112 - val_loss: 0.3304 - val_acc: 0.9329\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3705 - acc: 0.9193 - val_loss: 0.3317 - val_acc: 0.9340\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3454 - acc: 0.9244 - val_loss: 0.3296 - val_acc: 0.9348\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3250 - acc: 0.9287 - val_loss: 0.3144 - val_acc: 0.9363\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9328\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3056 - acc: 0.9328 - val_loss: 0.3236 - val_acc: 0.9352\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9372\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.2855 - acc: 0.9372 - val_loss: 0.3468 - val_acc: 0.9341\n",
      "Epoch 00008: early stopping\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.7085 - acc: 0.8393 - val_loss: 0.4582 - val_acc: 0.9017\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.4674 - acc: 0.8984 - val_loss: 0.3362 - val_acc: 0.9303\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3881 - acc: 0.9154 - val_loss: 0.3392 - val_acc: 0.9331\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.9222\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3556 - acc: 0.9222 - val_loss: 0.3341 - val_acc: 0.9331\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3251 - acc: 0.9286 - val_loss: 0.3299 - val_acc: 0.9336\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.9298\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3189 - acc: 0.9298 - val_loss: 0.3338 - val_acc: 0.9329\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3137 - acc: 0.9310 - val_loss: 0.3310 - val_acc: 0.9340\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9311\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3129 - acc: 0.9311 - val_loss: 0.3324 - val_acc: 0.9337\n",
      "Epoch 9/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9313\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "250/250 [==============================] - 59s 237ms/step - loss: 0.3125 - acc: 0.9313 - val_loss: 0.3314 - val_acc: 0.9341\n",
      "Epoch 10/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9313\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3121 - acc: 0.9313 - val_loss: 0.3320 - val_acc: 0.9339\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3120 - acc: 0.9314 - val_loss: 0.3302 - val_acc: 0.9342\n",
      "Epoch 12/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9312\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3127 - acc: 0.9313 - val_loss: 0.3320 - val_acc: 0.9340\n",
      "Epoch 13/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.9311\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.99999943962493e-12.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3129 - acc: 0.9312 - val_loss: 0.3312 - val_acc: 0.9341\n",
      "Epoch 00013: early stopping\n",
      "\n",
      "TESTING MODEL: unet_ternaus\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.4830 - acc: 0.8974 - val_loss: 0.3238 - val_acc: 0.93153s - loss: 0.4\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.9230\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.3522 - acc: 0.9231 - val_loss: 0.3527 - val_acc: 0.9289\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3144 - acc: 0.9311 - val_loss: 0.3629 - val_acc: 0.9332\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.3053 - acc: 0.9331 - val_loss: 0.3684 - val_acc: 0.9334\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9347\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2982 - acc: 0.9346 - val_loss: 0.3833 - val_acc: 0.9317\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9358\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2932 - acc: 0.9357 - val_loss: 0.3821 - val_acc: 0.9323\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 63s 251ms/step - loss: 0.5030 - acc: 0.8962 - val_loss: 0.3564 - val_acc: 0.9285\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.3637 - acc: 0.9206 - val_loss: 0.3605 - val_acc: 0.9321\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3195 - acc: 0.9299 - val_loss: 0.3423 - val_acc: 0.9333\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9362\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.2903 - acc: 0.9362 - val_loss: 0.3769 - val_acc: 0.9302\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9409\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2690 - acc: 0.9409 - val_loss: 0.3911 - val_acc: 0.9323\n",
      "Epoch 00005: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 63s 254ms/step - loss: 0.5618 - acc: 0.8883 - val_loss: 0.3483 - val_acc: 0.9276\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3723 - acc: 0.9189 - val_loss: 0.3294 - val_acc: 0.9339\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.9292\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3238 - acc: 0.9292 - val_loss: 0.3310 - val_acc: 0.9332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9354\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.2952 - acc: 0.9354 - val_loss: 0.3519 - val_acc: 0.9333\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 0.4948 - acc: 0.8930 - val_loss: 0.3435 - val_acc: 0.9299\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.3665 - acc: 0.9199 - val_loss: 0.3405 - val_acc: 0.9327\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.9297\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.3202 - acc: 0.9297 - val_loss: 0.3510 - val_acc: 0.9277\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.2904 - acc: 0.9362 - val_loss: 0.3688 - val_acc: 0.9333\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.2823 - acc: 0.9380 - val_loss: 0.3596 - val_acc: 0.9336\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9390\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.2775 - acc: 0.9390 - val_loss: 0.3824 - val_acc: 0.9326\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9399\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.2735 - acc: 0.9399 - val_loss: 0.3815 - val_acc: 0.9321\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 63s 250ms/step - loss: 0.6466 - acc: 0.8832 - val_loss: 0.3660 - val_acc: 0.9245\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3871 - acc: 0.9162 - val_loss: 0.3524 - val_acc: 0.9275\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.3395 - acc: 0.9259 - val_loss: 0.3633 - val_acc: 0.9309\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 59s 234ms/step - loss: 0.3072 - acc: 0.9328 - val_loss: 0.3627 - val_acc: 0.9311\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9380\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2831 - acc: 0.9380 - val_loss: 0.3820 - val_acc: 0.9297\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9425\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2622 - acc: 0.9425 - val_loss: 0.3900 - val_acc: 0.9309\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "TESTING MODEL: unet_ternaus_drop\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.4732 - acc: 0.8991 - val_loss: 0.3247 - val_acc: 0.9316\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 237ms/step - loss: 0.3509 - acc: 0.9233 - val_loss: 0.3264 - val_acc: 0.9346\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.9310\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3137 - acc: 0.9311 - val_loss: 0.3479 - val_acc: 0.9345\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9374\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.2848 - acc: 0.9374 - val_loss: 0.3561 - val_acc: 0.9344\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 0.5371 - acc: 0.8865 - val_loss: 0.3338 - val_acc: 0.9312\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3735 - acc: 0.9180 - val_loss: 0.3142 - val_acc: 0.9350\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.9273\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3309 - acc: 0.9273 - val_loss: 0.3373 - val_acc: 0.9308\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9340\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.2998 - acc: 0.9340 - val_loss: 0.3419 - val_acc: 0.9324\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 0.5711 - acc: 0.8837 - val_loss: 0.3519 - val_acc: 0.9282\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3803 - acc: 0.9172 - val_loss: 0.3211 - val_acc: 0.9353\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.9262\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3375 - acc: 0.9262 - val_loss: 0.4099 - val_acc: 0.9315\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9321\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3087 - acc: 0.9321 - val_loss: 0.3570 - val_acc: 0.9349\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 256ms/step - loss: 0.5839 - acc: 0.8855 - val_loss: 0.3495 - val_acc: 0.9274\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3842 - acc: 0.9170 - val_loss: 0.3387 - val_acc: 0.9308\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3406 - acc: 0.9257 - val_loss: 0.3560 - val_acc: 0.9328\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9308\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3160 - acc: 0.9308 - val_loss: 0.3601 - val_acc: 0.9296\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9362\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.2908 - acc: 0.9362 - val_loss: 0.3707 - val_acc: 0.9296\n",
      "Epoch 00005: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 0.4705 - acc: 0.8979 - val_loss: 0.3550 - val_acc: 0.9262\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.3547 - acc: 0.9224 - val_loss: 0.3186 - val_acc: 0.9339\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9302\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3179 - acc: 0.9302 - val_loss: 0.3428 - val_acc: 0.9333\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.2857 - acc: 0.9372 - val_loss: 0.3537 - val_acc: 0.9347\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9387\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.2788 - acc: 0.9387 - val_loss: 0.3643 - val_acc: 0.9338\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9396\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.2747 - acc: 0.9396 - val_loss: 0.3653 - val_acc: 0.9339\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "TESTING MODEL: unet_ternaus_bn\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 76s 303ms/step - loss: 0.3857 - acc: 0.9167 - val_loss: 0.3476 - val_acc: 0.9266\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2895 - acc: 0.9367 - val_loss: 0.3478 - val_acc: 0.9285\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 66s 264ms/step - loss: 0.2546 - acc: 0.9444 - val_loss: 0.3507 - val_acc: 0.9287\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 66s 264ms/step - loss: 0.2307 - acc: 0.9497 - val_loss: 0.3494 - val_acc: 0.9301\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 66s 264ms/step - loss: 0.2131 - acc: 0.9536 - val_loss: 0.3583 - val_acc: 0.9305\n",
      "Epoch 6/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9568\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 261ms/step - loss: 0.1984 - acc: 0.9568 - val_loss: 0.3687 - val_acc: 0.9284\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9610\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.1794 - acc: 0.9610 - val_loss: 0.3755 - val_acc: 0.9293\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 77s 310ms/step - loss: 0.4013 - acc: 0.9097 - val_loss: 0.3258 - val_acc: 0.9327\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9363\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2907 - acc: 0.9363 - val_loss: 0.3359 - val_acc: 0.9315\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9443\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 0.2545 - acc: 0.9443 - val_loss: 0.3440 - val_acc: 0.9307\n",
      "Epoch 00003: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 76s 305ms/step - loss: 0.3803 - acc: 0.9183 - val_loss: 0.3409 - val_acc: 0.9284\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2877 - acc: 0.9373 - val_loss: 0.3409 - val_acc: 0.9291\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2520 - acc: 0.9450 - val_loss: 0.3386 - val_acc: 0.9311\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9505\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2268 - acc: 0.9506 - val_loss: 0.3656 - val_acc: 0.9279\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9558\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2033 - acc: 0.9558 - val_loss: 0.3634 - val_acc: 0.9295\n",
      "Epoch 00005: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 77s 308ms/step - loss: 0.3786 - acc: 0.9177 - val_loss: 0.3118 - val_acc: 0.9350\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9366\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2894 - acc: 0.9366 - val_loss: 0.3331 - val_acc: 0.9322\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9451\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2512 - acc: 0.9452 - val_loss: 0.3350 - val_acc: 0.9319\n",
      "Epoch 00003: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 78s 313ms/step - loss: 0.3794 - acc: 0.9176 - val_loss: 0.3322 - val_acc: 0.9295\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9366\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2899 - acc: 0.9367 - val_loss: 0.3458 - val_acc: 0.9272\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2528 - acc: 0.9449 - val_loss: 0.3368 - val_acc: 0.9304\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9474\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2412 - acc: 0.9474 - val_loss: 0.3439 - val_acc: 0.9294\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9485\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2360 - acc: 0.9485 - val_loss: 0.3437 - val_acc: 0.9304\n",
      "Epoch 00005: early stopping\n",
      "\n",
      "TESTING MODEL: unet_ternaus_dropbn\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 77s 308ms/step - loss: 0.3901 - acc: 0.9159 - val_loss: 0.3256 - val_acc: 0.9350\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9358\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2930 - acc: 0.9358 - val_loss: 0.3478 - val_acc: 0.9291\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9435\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2579 - acc: 0.9435 - val_loss: 0.3418 - val_acc: 0.9316\n",
      "Epoch 00003: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 78s 312ms/step - loss: 0.3932 - acc: 0.9143 - val_loss: 0.3342 - val_acc: 0.9308\n",
      "Epoch 2/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.9347\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2983 - acc: 0.9347 - val_loss: 0.3454 - val_acc: 0.9268\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2632 - acc: 0.9423 - val_loss: 0.3285 - val_acc: 0.9312\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9447\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2526 - acc: 0.9447 - val_loss: 0.3370 - val_acc: 0.9300\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9457\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 0.2482 - acc: 0.9457 - val_loss: 0.3346 - val_acc: 0.9304\n",
      "Epoch 00005: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 77s 310ms/step - loss: 0.4004 - acc: 0.9115 - val_loss: 0.3275 - val_acc: 0.9321\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 66s 264ms/step - loss: 0.2960 - acc: 0.9353 - val_loss: 0.3148 - val_acc: 0.9340\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9427\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2620 - acc: 0.9427 - val_loss: 0.3313 - val_acc: 0.9335\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9493\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2324 - acc: 0.9493 - val_loss: 0.3347 - val_acc: 0.9331\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.8517 - acc: 0.8733 - val_loss: 0.3558 - val_acc: 0.9256\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 66s 263ms/step - loss: 0.3321 - acc: 0.9275 - val_loss: 0.3568 - val_acc: 0.9260\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2919 - acc: 0.9364 - val_loss: 0.3425 - val_acc: 0.9295\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 66s 266ms/step - loss: 0.2627 - acc: 0.9428 - val_loss: 0.3407 - val_acc: 0.9303\n",
      "Epoch 5/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9475\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2412 - acc: 0.9475 - val_loss: 0.3508 - val_acc: 0.9270\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.2162 - acc: 0.9532 - val_loss: 0.3408 - val_acc: 0.9319\n",
      "Epoch 7/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9548\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2088 - acc: 0.9548 - val_loss: 0.3507 - val_acc: 0.9308\n",
      "Epoch 8/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9554\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2060 - acc: 0.9554 - val_loss: 0.3478 - val_acc: 0.9313\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.5512 - acc: 0.8928 - val_loss: 0.3472 - val_acc: 0.9273\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 66s 264ms/step - loss: 0.3024 - acc: 0.9340 - val_loss: 0.3505 - val_acc: 0.9302\n",
      "Epoch 3/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9417\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2669 - acc: 0.9417 - val_loss: 0.3515 - val_acc: 0.9295\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9481\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2383 - acc: 0.9481 - val_loss: 0.3510 - val_acc: 0.9282\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "test_title = '256px_250steps_20epochs'\n",
    "\n",
    "training_params = dict(\n",
    "    train_steps=250, \n",
    "    val_steps=100, \n",
    "    epochs=20, \n",
    "    iterations=5, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    save_pth = f'{pretrained_pth}{model[0]}_{test_title}.h5'\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, \n",
    "                         model_params=model[2], save_pth=save_pth)\n",
    "    results_df = hists2df(results)\n",
    "    results_df.to_csv(f'{results_pth}{model[0]}_{test_title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING MODEL: unet_baseline \n",
    "train_steps=1000\n",
    "epochs=5\n",
    "\n",
    "new augmentation\n",
    "\n",
    "loss: 0.4721 - acc: 0.8949 - val_loss: 0.3291 - val_acc: 0.9325\n",
    "loss: 0.3288 - acc: 0.9284 - val_loss: 0.3316 - val_acc: 0.9346\n",
    "loss: 0.2828 - acc: 0.9381 - val_loss: 0.3206 - val_acc: 0.9349\n",
    "loss: 0.2543 - acc: 0.9443 - val_loss: 0.3570 - val_acc: 0.9299  <--overfitting\n",
    "loss: 0.2304 - acc: 0.9496 - val_loss: 0.3599 - val_acc: 0.9341\n",
    "\n",
    "512px 2000 steps\n",
    "loss: 1.1966 - acc: 0.9010 - val_loss: 1.0991 - val_acc: 0.9367\n",
    "loss: 1.0032 - acc: 0.9439 - val_loss: 1.0317 - val_acc: 0.9312  <--overfitting\n",
    "\n",
    "old augmentation\n",
    "\n",
    ",val_loss,val_acc,loss,acc,experiment,epoch\n",
    "0,0.3938294525444508,0.9204465866088867,0.5577377219498157,0.8792923736572266,0.0,0.0\n",
    "1,0.3559911660850048,0.9349861907958984,0.4401548975408077,0.9053520278930665,0.0,1.0\n",
    "2,0.39109488159418104,0.9267212295532227,0.4238285449743271,0.9082808227539062,0.0,2.0    <--overfitting\n",
    "3,0.4155013260245323,0.9168938446044922,0.42954228895902635,0.9082791290283203,0.0,3.0\n",
    "4,0.36537827536463735,0.9267246627807617,0.39881821677088736,0.9130117721557617,0.0,4.0\n",
    "\n",
    "5,0.49226998195052146,0.9182594680786133,0.6562534253895282,0.8670453491210938,1.0,0.0\n",
    "6,0.4583278933167458,0.9273256683349609,0.554767794162035,0.8987517471313476,1.0,1.0\n",
    "7,0.4612529504299164,0.9287994384765625,0.5254129691720009,0.904376106262207,1.0,2.0\n",
    "8,0.4672836236655712,0.9274766540527344,0.5032809109687805,0.9079330139160157,1.0,3.0   <--overfitting\n",
    "9,0.4568671178817749,0.922571907043457,0.5079058838784695,0.9074051818847656,1.0,4.0\n",
    "\n",
    "10,1.3314954286813736,0.7966962432861329,1.3639041519165038,0.7678601226806641,2.0,0.0\n",
    "11,1.282408196926117,0.7967258834838867,1.3139923479557036,0.7705478591918945,2.0,1.0\n",
    "12,1.2392406225204469,0.7965563201904297,1.273618043422699,0.7686567840576172,2.0,2.0\n",
    "13,1.2005380243062973,0.7966357803344727,1.237180896639824,0.769509765625,2.0,3.0\n",
    "14,1.166281766295433,0.7965296936035157,1.2050516810417176,0.7705629653930665,2.0,4.0\n",
    "\n",
    "15,0.47512905225157737,0.9222694778442383,0.6407421391904354,0.8729847259521485,3.0,0.0\n",
    "16,0.4551257087290287,0.9326386260986328,0.5516852941811085,0.8992034530639649,3.0,1.0\n",
    "17,0.47955494463443754,0.9300589370727539,0.5159895688593388,0.9065925064086914,3.0,2.0 \n",
    "18,0.4419742250442505,0.9331255340576172,0.5083408271372318,0.9076029663085937,3.0,3.0\n",
    "19,0.4340150611102581,0.9320641326904296,0.4795567348897457,0.9116422729492187,3.0,4.0    <--overfitting\n",
    "\n",
    "20,0.37043760359287264,0.9277394485473632,0.5708116674721241,0.8749972534179687,4.0,0.0\n",
    "21,0.3530151304602623,0.9324607086181641,0.4412955792546272,0.9052258071899414,4.0,1.0\n",
    "22,0.3365512517094612,0.935040283203125,0.43223093220591546,0.9074309005737304,4.0,2.0\n",
    "23,0.3444555760920048,0.930106315612793,0.4199221305847168,0.9092439498901367,4.0,3.0   <--overfitting\n",
    "24,0.34377639517188074,0.9305371475219727,0.40034309843182564,0.9129037475585937,4.0,4.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TESTING MODEL: unet_ternaus\n",
    "train_steps=2000\n",
    "epochs=5\n",
    "\n",
    "loss: 0.3033 - acc: 0.9342 - val_loss: 0.3997 - val_acc: 0.9302\n",
    "loss: 0.1896 - acc: 0.9586 - val_loss: 0.4191 - val_acc: 0.9312\n",
    "loss: 0.1561 - acc: 0.9660 - val_loss: 0.4229 - val_acc: 0.9336\n",
    "loss: 0.1372 - acc: 0.9701 - val_loss: 0.4552 - val_acc: 0.9294  <--overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast test: 50 training steps\n",
    "\n",
    "## Compare baseline implementation to more recent u-net implementations\n",
    "\n",
    "\n",
    "TODO: update\n",
    "\n",
    "It is observed that the final accuracy of both models varies in the range 80-96% when trained for 5 epochs with 2000 iterations. How can we quickly compare the performance of these two models?\n",
    "\n",
    "Here's a quick comparison of models by training each for 50 training steps from scratch 30 times. We compare our baseline implementation to [zhixuhao's model](https://github.com/zhixuhao/unet) and find no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_baseline\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 306ms/step - loss: 0.8908 - acc: 0.7699 - val_loss: 0.6800 - val_acc: 0.7970\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 0.8965 - acc: 0.7747 - val_loss: 0.6447 - val_acc: 0.7966\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 1.3857 - acc: 0.7706 - val_loss: 1.3808 - val_acc: 0.7995\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 302ms/step - loss: 0.8787 - acc: 0.7674 - val_loss: 0.6696 - val_acc: 0.7970\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 0.8155 - acc: 0.7976 - val_loss: 0.6133 - val_acc: 0.8709\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 1.1616 - acc: 0.7671 - val_loss: 0.8798 - val_acc: 0.7972\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 0.9073 - acc: 0.7761 - val_loss: 0.6454 - val_acc: 0.7970\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 0.9560 - acc: 0.7843 - val_loss: 0.8572 - val_acc: 0.8449\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 299ms/step - loss: 1.0321 - acc: 0.7688 - val_loss: 0.8062 - val_acc: 0.7974\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 0.7930 - acc: 0.8087 - val_loss: 0.5695 - val_acc: 0.8610\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 1.4314 - acc: 0.7411 - val_loss: 1.3835 - val_acc: 0.7969\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 0.8692 - acc: 0.7717 - val_loss: 0.6716 - val_acc: 0.7970\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 0.9149 - acc: 0.7754 - val_loss: 0.6841 - val_acc: 0.7973\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 0.8442 - acc: 0.7979 - val_loss: 0.6866 - val_acc: 0.8618\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 0.9822 - acc: 0.7685 - val_loss: 0.6600 - val_acc: 0.7967\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 1.3880 - acc: 0.6990 - val_loss: 1.3834 - val_acc: 0.7948\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 1.4181 - acc: 0.7329 - val_loss: 1.3835 - val_acc: 0.7971\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 304ms/step - loss: 0.8002 - acc: 0.8098 - val_loss: 0.5820 - val_acc: 0.8735\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 306ms/step - loss: 1.0849 - acc: 0.7684 - val_loss: 0.9566 - val_acc: 0.7966\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 0.7550 - acc: 0.8233 - val_loss: 0.5837 - val_acc: 0.8794\n",
      "\n",
      "TESTING MODEL: unet_baseline_upsampled\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 365ms/step - loss: 1.3850 - acc: 0.7677 - val_loss: 1.3833 - val_acc: 0.7971\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 0.8873 - acc: 0.7749 - val_loss: 0.6650 - val_acc: 0.8647\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 1.0160 - acc: 0.7693 - val_loss: 0.6826 - val_acc: 0.7966\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 0.8618 - acc: 0.7684 - val_loss: 0.6567 - val_acc: 0.7967\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 1.3871 - acc: 0.7572 - val_loss: 1.3834 - val_acc: 0.7969\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 0.9328 - acc: 0.7770 - val_loss: 0.6628 - val_acc: 0.7970\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 0.8931 - acc: 0.7976 - val_loss: 0.6060 - val_acc: 0.8564\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 358ms/step - loss: 0.9570 - acc: 0.7797 - val_loss: 0.7258 - val_acc: 0.7968\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 1.3785 - acc: 0.7709 - val_loss: 1.3755 - val_acc: 0.7969\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 352ms/step - loss: 1.3959 - acc: 0.7681 - val_loss: 1.3834 - val_acc: 0.7967\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 357ms/step - loss: 1.0597 - acc: 0.7667 - val_loss: 0.8882 - val_acc: 0.7966\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 1.4315 - acc: 0.7600 - val_loss: 1.3836 - val_acc: 0.7972\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 0.9405 - acc: 0.7832 - val_loss: 0.7424 - val_acc: 0.8283\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 0.8261 - acc: 0.8038 - val_loss: 0.6081 - val_acc: 0.8666\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 360ms/step - loss: 0.9277 - acc: 0.7766 - val_loss: 0.6714 - val_acc: 0.8638\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 1.4414 - acc: 0.7539 - val_loss: 1.3836 - val_acc: 0.7966\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 1.3935 - acc: 0.7655 - val_loss: 1.3834 - val_acc: 0.7963\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 0.8275 - acc: 0.7857 - val_loss: 0.6177 - val_acc: 0.8599\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 0.9131 - acc: 0.7891 - val_loss: 0.6680 - val_acc: 0.8316\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 1.2061 - acc: 0.7695 - val_loss: 0.9295 - val_acc: 0.7967\n",
      "\n",
      "TESTING MODEL: unet_pix2pix_paper\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 291ms/step - loss: 12.8848 - acc: 0.5017 - val_loss: 12.0437 - val_acc: 0.5380\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 291ms/step - loss: 13.9930 - acc: 0.4832 - val_loss: 12.9554 - val_acc: 0.5135\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 299ms/step - loss: 17.9974 - acc: 0.3626 - val_loss: 16.7740 - val_acc: 0.3991\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 292ms/step - loss: 9.8925 - acc: 0.5830 - val_loss: 8.8484 - val_acc: 0.6234\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 9.8717 - acc: 0.5948 - val_loss: 9.4111 - val_acc: 0.6188\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 9.9197 - acc: 0.5927 - val_loss: 8.9877 - val_acc: 0.6256\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 291ms/step - loss: 10.3759 - acc: 0.5596 - val_loss: 9.6778 - val_acc: 0.5866\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 14s 289ms/step - loss: 15.2947 - acc: 0.4509 - val_loss: 14.1459 - val_acc: 0.4917\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 16.8514 - acc: 0.3992 - val_loss: 15.8331 - val_acc: 0.4322\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 9.3629 - acc: 0.6150 - val_loss: 8.8395 - val_acc: 0.6378\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 13.7798 - acc: 0.4826 - val_loss: 12.9108 - val_acc: 0.5138\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 13.4924 - acc: 0.5020 - val_loss: 13.2910 - val_acc: 0.5166\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 296ms/step - loss: 17.5216 - acc: 0.3865 - val_loss: 15.5826 - val_acc: 0.4486\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 290ms/step - loss: 10.9709 - acc: 0.5483 - val_loss: 10.7956 - val_acc: 0.5649\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 10.0806 - acc: 0.5874 - val_loss: 9.1718 - val_acc: 0.6194\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 292ms/step - loss: 11.9659 - acc: 0.5445 - val_loss: 11.3789 - val_acc: 0.5679\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 8.0595 - acc: 0.6292 - val_loss: 7.5102 - val_acc: 0.6537\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 291ms/step - loss: 20.1244 - acc: 0.3370 - val_loss: 19.0430 - val_acc: 0.3744\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 9.1580 - acc: 0.6122 - val_loss: 8.2529 - val_acc: 0.6432\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 11.4983 - acc: 0.5472 - val_loss: 11.1451 - val_acc: 0.5676\n",
      "\n",
      "TESTING MODEL: unet_pix2pix_pytorch\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 1.3286 - acc: 0.7557 - val_loss: 1.0400 - val_acc: 0.7837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50/50 [==============================] - 11s 229ms/step - loss: 2.1285 - acc: 0.5346 - val_loss: 1.1335 - val_acc: 0.7217\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 25.1308 - acc: 0.2204 - val_loss: 25.6850 - val_acc: 0.2032\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 237ms/step - loss: 2.2256 - acc: 0.6784 - val_loss: 1.2907 - val_acc: 0.7629\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 1.6083 - acc: 0.7578 - val_loss: 1.3574 - val_acc: 0.7730\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 2.7300 - acc: 0.5509 - val_loss: 1.1893 - val_acc: 0.7002\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 25.1286 - acc: 0.2205 - val_loss: 25.6869 - val_acc: 0.2032\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 25.1635 - acc: 0.2194 - val_loss: 25.6768 - val_acc: 0.2035\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 231ms/step - loss: 25.0982 - acc: 0.2214 - val_loss: 25.6831 - val_acc: 0.2033\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 6.3645 - acc: 0.4748 - val_loss: 2.6211 - val_acc: 0.6276\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 235ms/step - loss: 2.7227 - acc: 0.5691 - val_loss: 1.2265 - val_acc: 0.7038\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 25.0962 - acc: 0.2215 - val_loss: 25.6741 - val_acc: 0.2036\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 25.1002 - acc: 0.2214 - val_loss: 25.7010 - val_acc: 0.2027\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 6.0236 - acc: 0.2785 - val_loss: 2.3743 - val_acc: 0.3181\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 17.2612 - acc: 0.2203 - val_loss: 7.5122 - val_acc: 0.2032\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 3.6290 - acc: 0.4482 - val_loss: 1.5160 - val_acc: 0.6067\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 2.2398 - acc: 0.6249 - val_loss: 1.2624 - val_acc: 0.7514\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 232ms/step - loss: 1.9110 - acc: 0.7562 - val_loss: 1.6048 - val_acc: 0.7724\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 9.4019 - acc: 0.3805 - val_loss: 5.2264 - val_acc: 0.4687\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 12s 234ms/step - loss: 1.2015 - acc: 0.7292 - val_loss: 1.0263 - val_acc: 0.7813\n",
      "\n",
      "TESTING MODEL: unet_ternaus_nopre\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 1.0375 - acc: 0.7776 - val_loss: 0.9150 - val_acc: 0.79663s - loss: 1.0564\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 0.9963 - acc: 0.7803 - val_loss: 0.8551 - val_acc: 0.7968\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 1.0631 - acc: 0.7732 - val_loss: 1.0024 - val_acc: 0.7967\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 1.0047 - acc: 0.7737 - val_loss: 0.8991 - val_acc: 0.8002\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 1.0007 - acc: 0.7807 - val_loss: 0.8577 - val_acc: 0.7968\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 1.0485 - acc: 0.7520 - val_loss: 1.2624 - val_acc: 0.5940\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 1.0235 - acc: 0.7794 - val_loss: 0.9036 - val_acc: 0.7969\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 0.9966 - acc: 0.7756 - val_loss: 0.7973 - val_acc: 0.7990\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 1.0452 - acc: 0.7681 - val_loss: 0.9068 - val_acc: 0.7968\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 0.9923 - acc: 0.7682 - val_loss: 0.8270 - val_acc: 0.7978\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.0507 - acc: 0.7814 - val_loss: 0.9935 - val_acc: 0.7967\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.9741 - acc: 0.7806 - val_loss: 0.8060 - val_acc: 0.79775 - acc: \n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 386ms/step - loss: 1.0276 - acc: 0.7687 - val_loss: 0.8849 - val_acc: 0.7969\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 0.9992 - acc: 0.7791 - val_loss: 0.8235 - val_acc: 0.7973\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 1.0318 - acc: 0.7674 - val_loss: 0.8667 - val_acc: 0.7968\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.0129 - acc: 0.7707 - val_loss: 0.8713 - val_acc: 0.7964c: 0.77\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.0234 - acc: 0.7655 - val_loss: 0.8523 - val_acc: 0.7966\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.0555 - acc: 0.7613 - val_loss: 0.9150 - val_acc: 0.7967\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 0.9998 - acc: 0.7772 - val_loss: 0.8121 - val_acc: 0.7973\n",
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 1.0304 - acc: 0.7689 - val_loss: 0.8882 - val_acc: 0.7965\n",
      "\n",
      "TESTING MODEL: unet_ternaus\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.9761 - acc: 0.8308 - val_loss: 0.4633 - val_acc: 0.9009\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.7522 - acc: 0.8550 - val_loss: 0.4309 - val_acc: 0.9057\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.6031 - acc: 0.8741 - val_loss: 0.3785 - val_acc: 0.9203\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.8435 - acc: 0.8401 - val_loss: 0.4455 - val_acc: 0.9039\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.8349 - acc: 0.8295 - val_loss: 0.4513 - val_acc: 0.9025\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 0.9014 - acc: 0.8104 - val_loss: 0.6570 - val_acc: 0.8573\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 0.9936 - acc: 0.8276 - val_loss: 0.4889 - val_acc: 0.8959\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 0.8438 - acc: 0.8135 - val_loss: 0.5761 - val_acc: 0.8725\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 0.7577 - acc: 0.8465 - val_loss: 0.4103 - val_acc: 0.9114\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.7581 - acc: 0.8378 - val_loss: 0.4722 - val_acc: 0.8999\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.6973 - acc: 0.8599 - val_loss: 0.4438 - val_acc: 0.9058\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.6792 - acc: 0.8596 - val_loss: 0.4228 - val_acc: 0.9103\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.6378 - acc: 0.8647 - val_loss: 0.3957 - val_acc: 0.9150\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.7426 - acc: 0.8490 - val_loss: 0.4223 - val_acc: 0.9093\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.1451 - acc: 0.8224 - val_loss: 0.4709 - val_acc: 0.8979\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.7899 - acc: 0.8532 - val_loss: 0.4417 - val_acc: 0.9056\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 375ms/step - loss: 0.8211 - acc: 0.8389 - val_loss: 0.4240 - val_acc: 0.9083\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.9946 - acc: 0.8381 - val_loss: 0.4431 - val_acc: 0.9045\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.9238 - acc: 0.8424 - val_loss: 0.4542 - val_acc: 0.9007\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.7763 - acc: 0.8335 - val_loss: 0.4385 - val_acc: 0.9093\n",
      "\n",
      "TESTING MODEL: unet_ternaus_drop\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 395ms/step - loss: 0.8663 - acc: 0.8457 - val_loss: 0.4215 - val_acc: 0.9101\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 0.8238 - acc: 0.8428 - val_loss: 0.4482 - val_acc: 0.9035\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 0.8806 - acc: 0.8186 - val_loss: 0.5328 - val_acc: 0.8816\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 1.4447 - acc: 0.8171 - val_loss: 0.5019 - val_acc: 0.8918\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 0.7752 - acc: 0.8328 - val_loss: 0.5382 - val_acc: 0.8794\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 1.0374 - acc: 0.8278 - val_loss: 0.4310 - val_acc: 0.9074\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 389ms/step - loss: 0.7006 - acc: 0.8554 - val_loss: 0.4189 - val_acc: 0.9101\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 0.6523 - acc: 0.8633 - val_loss: 0.4014 - val_acc: 0.9148\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 0.6813 - acc: 0.8587 - val_loss: 0.4141 - val_acc: 0.9102\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 0.7244 - acc: 0.8533 - val_loss: 0.4340 - val_acc: 0.9077\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 391ms/step - loss: 0.7408 - acc: 0.8454 - val_loss: 0.4527 - val_acc: 0.9044\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 0.8534 - acc: 0.8388 - val_loss: 0.4238 - val_acc: 0.9100\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 394ms/step - loss: 0.8178 - acc: 0.8457 - val_loss: 0.4326 - val_acc: 0.9073\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 391ms/step - loss: 0.9400 - acc: 0.8307 - val_loss: 0.4587 - val_acc: 0.9014\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 394ms/step - loss: 0.9048 - acc: 0.8293 - val_loss: 0.4802 - val_acc: 0.8933\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 0.7985 - acc: 0.8329 - val_loss: 0.4844 - val_acc: 0.8940\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 0.7377 - acc: 0.8458 - val_loss: 0.4440 - val_acc: 0.9047s - loss: 0.7420 - acc: 0.845\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 390ms/step - loss: 0.6796 - acc: 0.8580 - val_loss: 0.4593 - val_acc: 0.9019\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 1.0102 - acc: 0.8264 - val_loss: 0.4545 - val_acc: 0.9004\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 0.6076 - acc: 0.8703 - val_loss: 0.4107 - val_acc: 0.9157\n",
      "\n",
      "TESTING MODEL: unet_ternaus_bn\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.5568 - acc: 0.8855 - val_loss: 0.3605 - val_acc: 0.9246\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.5838 - acc: 0.8719 - val_loss: 0.5524 - val_acc: 0.8769\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 1.4444 - acc: 0.7761 - val_loss: 0.3914 - val_acc: 0.9186\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 0.5710 - acc: 0.8671 - val_loss: 0.3705 - val_acc: 0.9201\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 1.5457 - acc: 0.7776 - val_loss: 0.3901 - val_acc: 0.9148\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 481ms/step - loss: 1.3528 - acc: 0.7880 - val_loss: 0.3996 - val_acc: 0.9119\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 1.4851 - acc: 0.7917 - val_loss: 0.3855 - val_acc: 0.9146\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 3.0095 - acc: 0.7276 - val_loss: 0.4062 - val_acc: 0.9167\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.5429 - acc: 0.8762 - val_loss: 0.3970 - val_acc: 0.9180\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.8804 - acc: 0.8090 - val_loss: 0.4006 - val_acc: 0.9153\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.5167 - acc: 0.8884 - val_loss: 0.3675 - val_acc: 0.9223\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.5381 - acc: 0.8800 - val_loss: 0.3578 - val_acc: 0.9252\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.5085 - acc: 0.8902 - val_loss: 0.3545 - val_acc: 0.9262\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 483ms/step - loss: 0.5487 - acc: 0.8865 - val_loss: 0.3839 - val_acc: 0.9207\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.5329 - acc: 0.8862 - val_loss: 0.3679 - val_acc: 0.9199\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 483ms/step - loss: 0.7488 - acc: 0.8310 - val_loss: 0.4007 - val_acc: 0.9138\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.5793 - acc: 0.8858 - val_loss: 0.3665 - val_acc: 0.9214\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.6580 - acc: 0.8558 - val_loss: 0.3628 - val_acc: 0.9223\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.6059 - acc: 0.8786 - val_loss: 0.3828 - val_acc: 0.9187\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.6359 - acc: 0.8576 - val_loss: 0.3630 - val_acc: 0.9236\n",
      "\n",
      "TESTING MODEL: unet_ternaus_dropbn\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 2.0594 - acc: 0.7517 - val_loss: 0.3868 - val_acc: 0.9172\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.5376 - acc: 0.8772 - val_loss: 0.3514 - val_acc: 0.9258\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.5212 - acc: 0.8871 - val_loss: 0.3468 - val_acc: 0.9260\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.5477 - acc: 0.8732 - val_loss: 0.3440 - val_acc: 0.9275\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 490ms/step - loss: 1.9677 - acc: 0.7782 - val_loss: 0.3709 - val_acc: 0.9195\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.5386 - acc: 0.8782 - val_loss: 0.3482 - val_acc: 0.9257\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.8004 - acc: 0.8283 - val_loss: 0.3472 - val_acc: 0.9262\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.5986 - acc: 0.8596 - val_loss: 0.3756 - val_acc: 0.9201\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.6009 - acc: 0.8614 - val_loss: 0.3649 - val_acc: 0.9222\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 494ms/step - loss: 0.6326 - acc: 0.8500 - val_loss: 0.4426 - val_acc: 0.9031\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 0.5420 - acc: 0.8747 - val_loss: 0.3706 - val_acc: 0.9208\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 0.6392 - acc: 0.8501 - val_loss: 0.4602 - val_acc: 0.8967\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 1.1824 - acc: 0.7975 - val_loss: 0.3969 - val_acc: 0.9129\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 491ms/step - loss: 0.7935 - acc: 0.8712 - val_loss: 0.3822 - val_acc: 0.9196\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 491ms/step - loss: 0.5554 - acc: 0.8696 - val_loss: 0.3680 - val_acc: 0.9240\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 490ms/step - loss: 0.5163 - acc: 0.8874 - val_loss: 0.3532 - val_acc: 0.9250\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.5105 - acc: 0.8892 - val_loss: 0.4156 - val_acc: 0.9189\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 490ms/step - loss: 0.8983 - acc: 0.8122 - val_loss: 0.3701 - val_acc: 0.9194\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 494ms/step - loss: 0.5512 - acc: 0.8720 - val_loss: 0.3709 - val_acc: 0.9209\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.5145 - acc: 0.8866 - val_loss: 0.3676 - val_acc: 0.9215\n"
     ]
    }
   ],
   "source": [
    "test_title = '256px_50steps'\n",
    "\n",
    "training_params = dict(\n",
    "    train_steps=50, \n",
    "    val_steps=100, \n",
    "    epochs=1, \n",
    "    iterations=20, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, \n",
    "                         model_params=model[2])\n",
    "    results_df = hists2df(results)\n",
    "    results_df.to_csv(f'{results_pth}{model[0]}_{test_title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation tuning\n",
    "\n",
    "## data_loader_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_test(model_fn, iterations, steps_per_iter, epochs=1, lr=1e-4):\n",
    "    hists = []\n",
    "    for i in range(iterations):\n",
    "        model = model_fn(input_size=(256, 256, 1))\n",
    "        model.compile(optimizer = Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit_generator(train_gen, steps_per_epoch=steps_per_iter, epochs=1)\n",
    "        hists.append(history.history)\n",
    "#         r = {k: r[k][-1] for k in r}\n",
    "#         df = df.append(r, ignore_index=True)\n",
    "        backend.clear_session()\n",
    "    return hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement test set loader\n",
    "\n",
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                     width_shift_range=0.05,\n",
    "                     height_shift_range=0.05,\n",
    "                     shear_range=0.05,\n",
    "                     zoom_range=0.05,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "# why is batch size 2? paper says bs=1\n",
    "train_gen = trainGenerator(2, train_pth, 'input', 'target', data_gen_args, save_to_dir=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      "1000/1000 [==============================] - 161s 161ms/step - loss: 0.5742 - acc: 0.8896\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 159s 159ms/step - loss: 0.5651 - acc: 0.8921\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 158s 158ms/step - loss: 0.4570 - acc: 0.8995\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2041a87435a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhists_baseline_slow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-273635125a8f>\u001b[0m in \u001b[0;36mfast_test\u001b[0;34m(model_fn, iterations, steps_per_iter, epochs, lr)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mhists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         r = {k: r[k][-1] for k in r}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2945\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2947\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2948\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2949\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    467\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    736\u001b[0m       \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hists_baseline_slow = fast_test(unet, 5, 1000, epochs=1, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sz = (256, 256, 1)\n",
    "batch_sz = 2\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "    ['unet_baseline', unet, dict(input_size=img_sz)],\n",
    "]\n",
    "\n",
    "# Data loaders\n",
    "train_loader = loader(train_pth, input_generator_train, target_generator_train, batch_sz=batch_sz, img_sz=img_sz[:2])\n",
    "test_loader = loader(test_pth, input_generator_test, target_generator_test, batch_sz=batch_sz, img_sz=img_sz[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_baseline\n",
      "Epoch 1/1\n",
      "Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      " 999/1000 [============================>.] - ETA: 0s - loss: 0.4675 - acc: 0.8973Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      "1000/1000 [==============================] - 166s 166ms/step - loss: 0.4675 - acc: 0.8973 - val_loss: 0.3348 - val_acc: 0.9324\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 0.4851 - acc: 0.8936 - val_loss: 0.3304 - val_acc: 0.9326\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 1.2724 - acc: 0.8682 - val_loss: 1.1669 - val_acc: 0.9347\n"
     ]
    }
   ],
   "source": [
    "training_params = dict(\n",
    "    train_steps=1000, \n",
    "    val_steps=200, \n",
    "    epochs=1, \n",
    "    iterations=3, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, model_params=model[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test A\n",
    "\n",
    "input_generator_train = ImageDataGenerator(\n",
    "     rotation_range=180,\n",
    "     width_shift_range=0.2,\n",
    "     height_shift_range=0.2,\n",
    "     brightness_range=[0.5, 1.5],\n",
    "     rescale=1./255,           #  rescale pixel vals 0-255 --> 0.0-1.0\n",
    "     shear_range=0.2,\n",
    "     zoom_range=[0.5, 1.0],\n",
    "     horizontal_flip=True,\n",
    "     fill_mode='reflect',\n",
    "     data_format='channels_last',\n",
    "     validation_split=0.0\n",
    " )\n",
    "\n",
    "loss: 0.5713 - acc: 0.8766 - val_loss: 0.3788 - val_acc: 0.9237\n",
    "loss: 0.6409 - acc: 0.8713 - val_loss: 0.4842 - val_acc: 0.9206\n",
    "loss: 1.2721 - acc: 0.8673 - val_loss: 1.1907 - val_acc: 0.9294\n",
    "\n",
    "\n",
    "### Test B\n",
    "\n",
    "input_generator_train = ImageDataGenerator(\n",
    "    rotation_range=0.2,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    #brightness_range=[0.5, 1.5],\n",
    "    rescale=1./255,           #  rescale pixel vals 0-255 --> 0.0-1.0\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest', #nearest\n",
    "    data_format='channels_last',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "loss: 0.4986 - acc: 0.8909 - val_loss: 0.3177 - val_acc: 0.9342\n",
    "loss: 1.3596 - acc: 0.7805 - val_loss: 1.3306 - val_acc: 0.7968\n",
    "loss: 0.4908 - acc: 0.8891 - val_loss: 0.3676 - val_acc: 0.9292\n",
    "\n",
    "\n",
    "### Test C\n",
    "\n",
    "as B:\n",
    "fill_mode='reflect'\n",
    "\n",
    "loss: 0.5527 - acc: 0.8786 - val_loss: 0.3485 - val_acc: 0.9337\n",
    "loss: 0.5636 - acc: 0.8933 - val_loss: 0.4527 - val_acc: 0.9340\n",
    "loss: 0.4779 - acc: 0.8942 - val_loss: 0.3359 - val_acc: 0.9326\n",
    "\n",
    "\n",
    "### Test D\n",
    "\n",
    "as C:\n",
    "rotation_range=0.0,\n",
    "\n",
    "loss: 0.5775 - acc: 0.8873 - val_loss: 0.4613 - val_acc: 0.9319\n",
    "loss: 1.2456 - acc: 0.8952 - val_loss: 1.1706 - val_acc: 0.9309\n",
    "loss: 0.4818 - acc: 0.8946 - val_loss: 0.3495 - val_acc: 0.9314\n",
    "\n",
    "### Test E\n",
    "\n",
    "as C\n",
    "brightness_range=[0.8, 1.2]\n",
    "\n",
    "loss: 0.4811 - acc: 0.8946 - val_loss: 0.3346 - val_acc: 0.9327\n",
    "loss: 0.4701 - acc: 0.8968 - val_loss: 0.4053 - val_acc: 0.9273\n",
    "loss: 0.4712 - acc: 0.8971 - val_loss: 0.3999 - val_acc: 0.9275\n",
    "\n",
    "### Test F\n",
    "\n",
    "as C\n",
    "rotation_range=2,\n",
    "brightness_range=[0.8, 1.2]\n",
    "\n",
    "loss: 0.4675 - acc: 0.8973 - val_loss: 0.3348 - val_acc: 0.9324\n",
    "loss: 0.4851 - acc: 0.8936 - val_loss: 0.3304 - val_acc: 0.9326\n",
    "loss: 1.2724 - acc: 0.8682 - val_loss: 1.1669 - val_acc: 0.9347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
