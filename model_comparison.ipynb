{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from utils import *\n",
    "from model.data_loader import *\n",
    "from model.unet_baseline import unet\n",
    "from model.octave_unet import o_unet\n",
    "from model.unet_ternaus import ternausNet16\n",
    "from model.unet_ternaus_tweaked import ternausNet16_tweaked\n",
    "from model.unet_pix2pix import unet_pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_pth = 'data/membrane/train'\n",
    "test_pth = 'data/membrane/test'\n",
    "results_pth = 'results/'\n",
    "pretrained_pth = 'pretrained/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment A – baseline\n",
    "\n",
    "Train the original unet model on the full 512px dataset until overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full available image size\n",
    "img_sz = (800, 600, 1)\n",
    "batch_sz = 2\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "    ['unet_baseline', unet, dict(input_size=img_sz, transpose=True)]\n",
    "]\n",
    "\n",
    "# Data loaders\n",
    "train_loader = loader(train_pth, input_generator_train, target_generator_train, \n",
    "                      batch_sz=batch_sz, img_sz=img_sz[:2])\n",
    "test_loader = loader(test_pth, input_generator_test, target_generator_test, \n",
    "                     batch_sz=batch_sz, img_sz=img_sz[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_baseline\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 259s 519ms/step - loss: 0.5889 - acc: 0.8872 - val_loss: 0.4561 - val_acc: 0.9299\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 257s 515ms/step - loss: 0.4870 - acc: 0.9201 - val_loss: 0.4700 - val_acc: 0.9347\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 257s 514ms/step - loss: 0.4551 - acc: 0.9270 - val_loss: 0.4470 - val_acc: 0.9349\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 257s 514ms/step - loss: 0.4327 - acc: 0.9318 - val_loss: 0.4386 - val_acc: 0.9376\n",
      "Epoch 5/10\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.9358\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "500/500 [==============================] - 255s 511ms/step - loss: 0.4139 - acc: 0.9358 - val_loss: 0.4374 - val_acc: 0.9367\n",
      "Epoch 6/10\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.9396\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "500/500 [==============================] - 255s 511ms/step - loss: 0.4003 - acc: 0.9396 - val_loss: 0.4717 - val_acc: 0.9371\n"
     ]
    }
   ],
   "source": [
    "# Results csv saved with this filename\n",
    "test_title = '512px_500steps_10epochs'\n",
    "\n",
    "# Experiment configuration\n",
    "training_params = dict(\n",
    "    train_steps=500, \n",
    "    val_steps=100, \n",
    "    epochs=10, \n",
    "    iterations=1, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Train models and record results\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    save_pth = f'{pretrained_pth}{model[0]}_{test_title}.h5'\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, \n",
    "                         model_params=model[2], save_pth=save_pth)\n",
    "    results_df = hists2df(results)\n",
    "    results_df.to_csv(f'{results_pth}{model[0]}_{test_title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Validation Accuracy: 0.9376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment B – Slow test\n",
    "Train all models under test for 20 epochs of 250 training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half available image size to reduce training time\n",
    "img_sz = (256, 256, 1)\n",
    "batch_sz = 2\n",
    "\n",
    "# Models\n",
    "models = [\n",
    "    ['unet_baseline',           unet,                 dict(input_size=img_sz, transpose=True)],\n",
    "    ['unet_baseline_upsampled', unet,                 dict(input_size=img_sz, transpose=False)],\n",
    "    ['unet_ternaus_nopre',      ternausNet16_tweaked, dict(input_size=img_sz, dropout=False, batch_norm=False, pretrained=False)],\n",
    "    ['unet_ternaus',            ternausNet16,         dict(input_size=img_sz)],\n",
    "    ['unet_ternaus_drop',       ternausNet16_tweaked, dict(input_size=img_sz, dropout=True,  batch_norm=False, pretrained=True)],\n",
    "    ['unet_ternaus_bn',         ternausNet16_tweaked, dict(input_size=img_sz, dropout=False, batch_norm=True,  pretrained=True)],\n",
    "    ['unet_ternaus_dropbn',     ternausNet16_tweaked, dict(input_size=img_sz, dropout=True,  batch_norm=True,  pretrained=True)],\n",
    "    ['unet_pix2pix',            unet_pix2pix, dict(input_size=img_sz)]\n",
    "]\n",
    "\n",
    "# Data loaders\n",
    "train_loader = loader(train_pth, input_generator_train, target_generator_train, batch_sz=batch_sz, img_sz=img_sz[:2])\n",
    "test_loader = loader(test_pth, input_generator_test, target_generator_test, batch_sz=batch_sz, img_sz=img_sz[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_pix2pix\n",
      "WARNING:tensorflow:From /home/paperspace/git/pix2pix/venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/paperspace/git/pix2pix/venv/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/paperspace/git/pix2pix/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      "Epoch 1/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 4.1364 - acc: 0.2600Found 30 images belonging to 1 classes.\n",
      "Found 30 images belonging to 1 classes.\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 2.6649 - acc: 0.3605\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 4.1305 - acc: 0.2605 - val_loss: 2.6649 - val_acc: 0.3605\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 1.1007 - acc: 0.7706\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 1.3803 - acc: 0.6194 - val_loss: 1.1007 - val_acc: 0.7706\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.8961 - acc: 0.8289\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.9906 - acc: 0.7967 - val_loss: 0.8961 - val_acc: 0.8289\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.7728 - acc: 0.8507\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.8399 - acc: 0.8233 - val_loss: 0.7728 - val_acc: 0.8507\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.7034 - acc: 0.8606\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.7521 - acc: 0.8369 - val_loss: 0.7034 - val_acc: 0.8606\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6637 - acc: 0.8684\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.6990 - acc: 0.8459 - val_loss: 0.6637 - val_acc: 0.8684\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.6354 - acc: 0.8729\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.6651 - acc: 0.8529 - val_loss: 0.6354 - val_acc: 0.8729\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.5968 - acc: 0.8797\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.6406 - acc: 0.8588 - val_loss: 0.5968 - val_acc: 0.8797\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6180 - acc: 0.8783\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.6206 - acc: 0.8640 - val_loss: 0.6180 - val_acc: 0.8783\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.5891 - acc: 0.8827\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.6126 - acc: 0.8663 - val_loss: 0.5891 - val_acc: 0.8827\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.5931 - acc: 0.8816\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.6109 - acc: 0.8667 - val_loss: 0.5931 - val_acc: 0.8816\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.5928 - acc: 0.8825\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 0.6090 - acc: 0.8673 - val_loss: 0.5928 - val_acc: 0.8825\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 1.4754 - acc: 0.4623\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 2.5571 - acc: 0.2938 - val_loss: 1.4754 - val_acc: 0.4623\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 1.0848 - acc: 0.7643\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 1.2515 - acc: 0.6421 - val_loss: 1.0848 - val_acc: 0.7643\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.8868 - acc: 0.8292\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.9695 - acc: 0.7952 - val_loss: 0.8868 - val_acc: 0.8292\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.7763 - acc: 0.8532\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.8256 - acc: 0.8231 - val_loss: 0.7763 - val_acc: 0.8532\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6865 - acc: 0.8635\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.7410 - acc: 0.8381 - val_loss: 0.6865 - val_acc: 0.8635\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.6613 - acc: 0.8729\n",
      "250/250 [==============================] - 37s 147ms/step - loss: 0.6885 - acc: 0.8484 - val_loss: 0.6613 - val_acc: 0.8729\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.6542 - acc: 0.8714\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.6543 - acc: 0.8560 - val_loss: 0.6542 - val_acc: 0.8714\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6150 - acc: 0.8792\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.6385 - acc: 0.8601 - val_loss: 0.6150 - val_acc: 0.8792\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6166 - acc: 0.8794\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.6354 - acc: 0.8604 - val_loss: 0.6166 - val_acc: 0.8794\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.6076 - acc: 0.8790\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.6331 - acc: 0.8614 - val_loss: 0.6076 - val_acc: 0.8790\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.6139 - acc: 0.8783\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.6298 - acc: 0.8623 - val_loss: 0.6139 - val_acc: 0.8783\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 1.5443 - acc: 0.3942\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 2.9194 - acc: 0.2672 - val_loss: 1.5443 - val_acc: 0.3942\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 1.1422 - acc: 0.7511\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 1.3096 - acc: 0.6064 - val_loss: 1.1422 - val_acc: 0.7511\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.9277 - acc: 0.8134\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 1.0264 - acc: 0.7836 - val_loss: 0.9277 - val_acc: 0.8134\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.8026 - acc: 0.8292\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.8820 - acc: 0.8060 - val_loss: 0.8026 - val_acc: 0.8292\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.7345 - acc: 0.8437\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.7955 - acc: 0.8198 - val_loss: 0.7345 - val_acc: 0.8437\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.6997 - acc: 0.8571\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 0.7382 - acc: 0.8311 - val_loss: 0.6997 - val_acc: 0.8571\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6606 - acc: 0.8650\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.6990 - acc: 0.8403 - val_loss: 0.6606 - val_acc: 0.8650\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6585 - acc: 0.8715\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.6680 - acc: 0.8491 - val_loss: 0.6585 - val_acc: 0.8715\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6218 - acc: 0.8758\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.6465 - acc: 0.8557 - val_loss: 0.6218 - val_acc: 0.8758\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.6285 - acc: 0.8750\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 33s 130ms/step - loss: 0.6278 - acc: 0.8612 - val_loss: 0.6285 - val_acc: 0.8750\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6075 - acc: 0.8802\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.6150 - acc: 0.8651 - val_loss: 0.6075 - val_acc: 0.8802\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.6071 - acc: 0.8817\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.6136 - acc: 0.8656 - val_loss: 0.6071 - val_acc: 0.8817\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.5938 - acc: 0.8816\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 33s 130ms/step - loss: 0.6125 - acc: 0.8659 - val_loss: 0.5938 - val_acc: 0.8816\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6045 - acc: 0.8810\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 33s 130ms/step - loss: 0.6155 - acc: 0.8649 - val_loss: 0.6045 - val_acc: 0.8810\n",
      "Epoch 00014: early stopping\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 1.5264 - acc: 0.4263\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 2.6101 - acc: 0.2864 - val_loss: 1.5264 - val_acc: 0.4263\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 1.1546 - acc: 0.7260\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 1.3031 - acc: 0.6027 - val_loss: 1.1546 - val_acc: 0.7260\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.9399 - acc: 0.8140\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 1.0292 - acc: 0.7765 - val_loss: 0.9399 - val_acc: 0.8140\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.8089 - acc: 0.8337\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.8849 - acc: 0.8085 - val_loss: 0.8089 - val_acc: 0.8337\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.7490 - acc: 0.8527\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.7956 - acc: 0.8237 - val_loss: 0.7490 - val_acc: 0.8527\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.7087 - acc: 0.8631\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.7365 - acc: 0.8354 - val_loss: 0.7087 - val_acc: 0.8631\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.6867 - acc: 0.8644\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.6951 - acc: 0.8447 - val_loss: 0.6867 - val_acc: 0.8644\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.6188 - acc: 0.8757\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 0.6619 - acc: 0.8533 - val_loss: 0.6188 - val_acc: 0.8757\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.6823 - acc: 0.8662\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 33s 133ms/step - loss: 0.6395 - acc: 0.8593 - val_loss: 0.6823 - val_acc: 0.8662\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.6005 - acc: 0.8791\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.6263 - acc: 0.8629 - val_loss: 0.6005 - val_acc: 0.8791\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.5856 - acc: 0.8828\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.6232 - acc: 0.8637 - val_loss: 0.5856 - val_acc: 0.8828\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5886 - acc: 0.8827\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 34s 134ms/step - loss: 0.6210 - acc: 0.8643 - val_loss: 0.5886 - val_acc: 0.8827\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5947 - acc: 0.8820\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.6221 - acc: 0.8644 - val_loss: 0.5947 - val_acc: 0.8820\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.8099 - acc: 0.2748\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 3.9861 - acc: 0.2326 - val_loss: 1.8099 - val_acc: 0.2748\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.3179 - acc: 0.6005\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 1.5143 - acc: 0.4369 - val_loss: 1.3179 - val_acc: 0.6005\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 1.0585 - acc: 0.7892\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 1.1667 - acc: 0.7210 - val_loss: 1.0585 - val_acc: 0.7892\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.9189 - acc: 0.8207\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.9829 - acc: 0.7928 - val_loss: 0.9189 - val_acc: 0.8207\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.8065 - acc: 0.8345\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.8712 - acc: 0.8091 - val_loss: 0.8065 - val_acc: 0.8345\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.7522 - acc: 0.8520\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.7934 - acc: 0.8233 - val_loss: 0.7522 - val_acc: 0.8520\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.7050 - acc: 0.8625\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.7359 - acc: 0.8353 - val_loss: 0.7050 - val_acc: 0.8625\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6762 - acc: 0.8685\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.6925 - acc: 0.8454 - val_loss: 0.6762 - val_acc: 0.8685\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.6236 - acc: 0.8753\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.6603 - acc: 0.8534 - val_loss: 0.6236 - val_acc: 0.8753\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.6012 - acc: 0.8811\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.6325 - acc: 0.8610 - val_loss: 0.6012 - val_acc: 0.8811\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.5703 - acc: 0.8860\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.6120 - acc: 0.8668 - val_loss: 0.5703 - val_acc: 0.8860\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5718 - acc: 0.8876\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.5903 - acc: 0.8730 - val_loss: 0.5718 - val_acc: 0.8876\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5561 - acc: 0.8906\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.5676 - acc: 0.8791 - val_loss: 0.5561 - val_acc: 0.8906\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5665 - acc: 0.8848: 1s - loss\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 0.5474 - acc: 0.8839 - val_loss: 0.5665 - val_acc: 0.8848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5203 - acc: 0.8974\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.5343 - acc: 0.8873 - val_loss: 0.5203 - val_acc: 0.8974\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.4996 - acc: 0.8992\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.5334 - acc: 0.8873 - val_loss: 0.4996 - val_acc: 0.8992\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.5021 - acc: 0.9007\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.5315 - acc: 0.8876 - val_loss: 0.5021 - val_acc: 0.9007\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5153 - acc: 0.8982\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.5297 - acc: 0.8879 - val_loss: 0.5153 - val_acc: 0.8982\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.4975 - acc: 0.9005\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "250/250 [==============================] - 33s 134ms/step - loss: 0.5283 - acc: 0.8882 - val_loss: 0.4975 - val_acc: 0.9005\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Results csv saved with this filename\n",
    "test_title = '256px_250steps_20epochs'\n",
    "\n",
    "# Experiment configuration\n",
    "training_params = dict(\n",
    "    train_steps=250, \n",
    "    val_steps=100, \n",
    "    epochs=20, \n",
    "    iterations=5, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Train models and record results\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    save_pth = f'{pretrained_pth}{model[0]}_{test_title}.h5'\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, \n",
    "                         model_params=model[2], save_pth=save_pth)\n",
    "    results_df = hists2df(results)\n",
    "    results_df.to_csv(f'{results_pth}{model[0]}_{test_title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment C – Fast test\n",
    "\n",
    "Train all models under test for 50 training steps. Repeat each experiment for 20 `iterations`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING MODEL: unet_pix2pix\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 5.8225 - acc: 0.2034\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 10.3476 - acc: 0.2237 - val_loss: 5.8225 - val_acc: 0.2034\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 3.0336 - acc: 0.2031\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 5.8625 - acc: 0.2231 - val_loss: 3.0336 - val_acc: 0.2031\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 4.5115 - acc: 0.2035\n",
      "50/50 [==============================] - 12s 249ms/step - loss: 9.3518 - acc: 0.2201 - val_loss: 4.5115 - val_acc: 0.2035\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 3.4247 - acc: 0.2032\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 5.8055 - acc: 0.2202 - val_loss: 3.4247 - val_acc: 0.2032\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 5.4437 - acc: 0.2034\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 9.4541 - acc: 0.2206 - val_loss: 5.4437 - val_acc: 0.2034\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 5.8741 - acc: 0.2033\n",
      "50/50 [==============================] - 13s 255ms/step - loss: 9.7477 - acc: 0.2206 - val_loss: 5.8741 - val_acc: 0.2033\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 4.1885 - acc: 0.2033\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 9.6231 - acc: 0.2216 - val_loss: 4.1885 - val_acc: 0.2033\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 3.2040 - acc: 0.2031\n",
      "50/50 [==============================] - 12s 249ms/step - loss: 6.6610 - acc: 0.2209 - val_loss: 3.2040 - val_acc: 0.2031\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 3.6220 - acc: 0.2036\n",
      "50/50 [==============================] - 12s 249ms/step - loss: 6.3022 - acc: 0.2194 - val_loss: 3.6220 - val_acc: 0.2036\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 3.7733 - acc: 0.2032\n",
      "50/50 [==============================] - 13s 252ms/step - loss: 7.1181 - acc: 0.2209 - val_loss: 3.7733 - val_acc: 0.2032\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 3.7046 - acc: 0.2030\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 6.7688 - acc: 0.2231 - val_loss: 3.7046 - val_acc: 0.2030\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 5.6521 - acc: 0.2031\n",
      "50/50 [==============================] - 13s 255ms/step - loss: 9.9922 - acc: 0.2187 - val_loss: 5.6521 - val_acc: 0.2031\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.9310 - acc: 0.2032\n",
      "50/50 [==============================] - 13s 251ms/step - loss: 7.8037 - acc: 0.2229 - val_loss: 2.9310 - val_acc: 0.2032\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 3.2042 - acc: 0.2036\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 6.3761 - acc: 0.2219 - val_loss: 3.2042 - val_acc: 0.2036\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 3.2645 - acc: 0.2030\n",
      "50/50 [==============================] - 13s 252ms/step - loss: 7.9495 - acc: 0.2212 - val_loss: 3.2645 - val_acc: 0.2030\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.7769 - acc: 0.2033\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 6.2316 - acc: 0.2219 - val_loss: 2.7769 - val_acc: 0.2033\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 2.8458 - acc: 0.2034\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 4.4399 - acc: 0.2209 - val_loss: 2.8458 - val_acc: 0.2034\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 3.2228 - acc: 0.2025\n",
      "50/50 [==============================] - 13s 256ms/step - loss: 5.2652 - acc: 0.2226 - val_loss: 3.2228 - val_acc: 0.2025\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 6.0160 - acc: 0.2035\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 8.5689 - acc: 0.2202 - val_loss: 6.0160 - val_acc: 0.2035\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 5.7268 - acc: 0.2031\n",
      "50/50 [==============================] - 13s 250ms/step - loss: 9.2893 - acc: 0.2209 - val_loss: 5.7268 - val_acc: 0.2031\n"
     ]
    }
   ],
   "source": [
    "# Results csv saved with this filename\n",
    "test_title = '256px_50steps'\n",
    "\n",
    "# Experiment configuration\n",
    "training_params = dict(\n",
    "    train_steps=50, \n",
    "    val_steps=100, \n",
    "    epochs=1, \n",
    "    iterations=20, \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Train models and record results\n",
    "for model in models:\n",
    "    print(f'\\nTESTING MODEL: {model[0]}')\n",
    "    results = test_model(model[1], train_loader, test_loader, **training_params, \n",
    "                         model_params=model[2])\n",
    "    results_df = hists2df(results)\n",
    "    results_df.to_csv(f'{results_pth}{model[0]}_{test_title}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter count\n",
    "\n",
    "Count parameters in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinitializing layer block1_conv1.kernel\n",
      "reinitializing layer block1_conv1.bias\n",
      "reinitializing layer block1_conv2.kernel\n",
      "reinitializing layer block1_conv2.bias\n",
      "reinitializing layer block2_conv1.kernel\n",
      "reinitializing layer block2_conv1.bias\n",
      "reinitializing layer block2_conv2.kernel\n",
      "reinitializing layer block2_conv2.bias\n",
      "reinitializing layer block3_conv1.kernel\n",
      "reinitializing layer block3_conv1.bias\n",
      "reinitializing layer block3_conv2.kernel\n",
      "reinitializing layer block3_conv2.bias\n",
      "reinitializing layer block3_conv3.kernel\n",
      "reinitializing layer block3_conv3.bias\n",
      "reinitializing layer block4_conv1.kernel\n",
      "reinitializing layer block4_conv1.bias\n",
      "reinitializing layer block4_conv2.kernel\n",
      "reinitializing layer block4_conv2.bias\n",
      "reinitializing layer block4_conv3.kernel\n",
      "reinitializing layer block4_conv3.bias\n",
      "reinitializing layer block5_conv1.kernel\n",
      "reinitializing layer block5_conv1.bias\n",
      "reinitializing layer block5_conv2.kernel\n",
      "reinitializing layer block5_conv2.bias\n",
      "reinitializing layer block5_conv3.kernel\n",
      "reinitializing layer block5_conv3.bias\n"
     ]
    }
   ],
   "source": [
    "# Count trainable parameters for each model\n",
    "params = []\n",
    "for model in models:\n",
    "    m = model[1](**model[2])\n",
    "    params.append([model[0], m.count_params()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unet_baseline</td>\n",
       "      <td>31031685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unet_baseline_upsampled</td>\n",
       "      <td>31031685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unet_ternaus_nopre</td>\n",
       "      <td>28470791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unet_ternaus</td>\n",
       "      <td>28467719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unet_ternaus_drop</td>\n",
       "      <td>28470791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unet_ternaus_bn</td>\n",
       "      <td>28495879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unet_ternaus_dropbn</td>\n",
       "      <td>28495879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unet_pix2pix</td>\n",
       "      <td>54417665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  parameters\n",
       "0            unet_baseline    31031685\n",
       "1  unet_baseline_upsampled    31031685\n",
       "2       unet_ternaus_nopre    28470791\n",
       "3             unet_ternaus    28467719\n",
       "4        unet_ternaus_drop    28470791\n",
       "5          unet_ternaus_bn    28495879\n",
       "6      unet_ternaus_dropbn    28495879\n",
       "7             unet_pix2pix    54417665"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(params, columns=['model', 'parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
